* Note
This is an updated version of the template directory which completes the first two parts of the first day of the workshop. It contains an updated version of the notebook with remote tracking, as well as code for building a remote web service running on AML compute.

Use this template if you feel that you will not have time to complete the preparations.

* MLOps Template
This is a template directory for people starting with the MLOps Workshop or the MLOps Master Class. Clone the template and follow the instructions below for how to setup your local environment. Then, read the instructions [[https://github.com/lukas-lundmark/mlops-masterclass/blob/main/day-1.org][here]] for how to continue.

* Getting Started

** Python Environment
You should have conda installed on your system.

Initialize the predefined conda environment in environment_setup

#+begin_src bash
conda env create -f environment_setup/ci_dependencies.yml -n mlopspython
conda activate mlopspython
#+end_src

** For M1 Users
Some of the dependencies do not have a binaries for M1 (arm64), so if you are running a Mac with an M1 chip you need to emulate an amd64 architecture. First, make sure that you have rosetta2 installed on your system. Then, you can run
#+begin_src bash
CONDA_SUBDIR=osx-64 conda env create -f environment_setup/ci_dependencies.yml -n mlopspython
conda activate mlopspython
#+end_src

** Azure CLI and Azure ML
Read the instructions [[https://github.com/lukas-lundmark/mlops-masterclass/blob/main/setup-azure.org][here]] for how to setup and configure Azure and Azure ML. After you have created your workspace, you should download the config.json object from the portal and save it to the root of this repo.

** Docker
You will need docker installed to run experiments and build deployments.
Go to https://docs.docker.com/get-docker/ and follow the instructions.

If you are unfamiliar with docker you can read the absolute basics [[https://github.com/lukas-lundmark/mlops-masterclass/blob/main/docker.org][here]].

** Downloading Data
Run the download data script from within the data directory which will download the training and test dataset for you.

#+begin_src bash
cd data
sh ./download-data.sh
#+end_src

* Experiment Tracking
You should start by inspecting the default notebook and modifying it to use proper experiment tracking
Start a notebook using command
#+begin_src bash
jupyter notebook
#+end_src
and open the notebook ~Exploratory Data Analysis with Tracking.ipynb~ in your browser or editor of choice. This notebook already contains the necessary code for starting an interactive experiment and registering datasets. Inspect the code and make sure you can run it from start to finish.

* Webservices
Use the  packaging script in ~ml_pipelines/deploy/package_service.py~ to build a local webservice.

Run the script and follow the instructions for how to build your docker image
#+begin_src bash
python -m ml_pipelines.deploy.package_service.py
#+end_src

There is a utility script ~send_request.py~ that you can use to send simple requests to your services. E.g.,
#+begin_src bash
python send_request.py --url http://localhost:6789/score --source-file data/diamonds-test.csv --n 10
#+end_src
which will send 10 records to your local service from the file data/diamonds-test.csv. Use this to test you different services.

Run the webservice build script to build a remote webservice running on AML Compute
#+begin_src bash
python -m ml_pipelines.deploy.build_webservice
#+end_src
or you can run it locally for debugging purposes

#+begin_src bash
python -m ml_pipelines.deploy.build_webservice --local
#+end_src
